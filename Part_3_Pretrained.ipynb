{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12489505,"sourceType":"datasetVersion","datasetId":7881424},{"sourceId":12489542,"sourceType":"datasetVersion","datasetId":7881453},{"sourceId":12489546,"sourceType":"datasetVersion","datasetId":7881456}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer-Based Title Generation - Part 3","metadata":{}},{"cell_type":"markdown","source":"In Part 3 of the project, we explore the use of a pretrained T5 sequence-to-sequence model for the task of book title generation based on book descriptions. This task is framed as a text summarization problem, where the model receives a book description as input and generates an appropriate title as output.\n  \nWe begin by loading the datasets that were prepared in Part 1 of the project. These include training, validation and test sets, each containing book descriptions and their corresponding titles. The data is preprocessed and tokenized to match the input format expected by the **T5 model**.\n  \nThe following components are used in the setup:  \n- Model and tokenizer: We use the **t5-small** variant loaded from the **Huggingface Model Hub** via  \ntokenizer = T5Tokenizer.from_pretrained('t5-small')  \nmodel = T5ForConditionalGeneration.from_pretrained('t5-small')  \n- Data Collation: We utilize **DataCollatorForSeq2Seq** to handle dynamic padding and ensure efficient batching during training and evaluation.\n- Evaluation Metric: The **ROUGE metric** is employed to evaluate the quality of the generated titles against the reference titles, with a focus on ROUGE-1.\n  \nFinally, we test the model on three examples from the test dataset, comparing the generated titles to the original ones to assess the model's performance.","metadata":{}},{"cell_type":"markdown","source":"## Load Data\n\nWe're importing the preprocessed datasets made in Part 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:06:39.119481Z","iopub.execute_input":"2025-07-20T08:06:39.119746Z","iopub.status.idle":"2025-07-20T08:06:40.917329Z","shell.execute_reply.started":"2025-07-20T08:06:39.119727Z","shell.execute_reply":"2025-07-20T08:06:40.916775Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/train-book-data/train_book_data.csv')\ndf_valid = pd.read_csv('/kaggle/input/valid-book-data/valid_book_data.csv')\ndf_test = pd.read_csv('/kaggle/input/test-book-data/test_book_data.csv')\n\ndf_train.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:06:42.509744Z","iopub.execute_input":"2025-07-20T08:06:42.510133Z","iopub.status.idle":"2025-07-20T08:06:42.615293Z","shell.execute_reply.started":"2025-07-20T08:06:42.510111Z","shell.execute_reply":"2025-07-20T08:06:42.614708Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              Title  \\\n1589                           howl and other poems   \n1590         crown of midnight (throne of glass #2)   \n1591      the cuckoo's calling (cormoran strike #1)   \n1592  saga, volume 2 (saga (collected editions) #2)   \n1593                             legend (legend #1)   \n\n                                            Description  \n1589  the prophetic poem, which was born by a genera...  \n1590  \"a line that should never be crossed is about ...  \n1591  a brilliant debut secret in a classic vein: a ...  \n1592  by the award-winning writer brian k. vaughan (...  \n1593  here there is an alternative title edition for...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1589</th>\n      <td>howl and other poems</td>\n      <td>the prophetic poem, which was born by a genera...</td>\n    </tr>\n    <tr>\n      <th>1590</th>\n      <td>crown of midnight (throne of glass #2)</td>\n      <td>\"a line that should never be crossed is about ...</td>\n    </tr>\n    <tr>\n      <th>1591</th>\n      <td>the cuckoo's calling (cormoran strike #1)</td>\n      <td>a brilliant debut secret in a classic vein: a ...</td>\n    </tr>\n    <tr>\n      <th>1592</th>\n      <td>saga, volume 2 (saga (collected editions) #2)</td>\n      <td>by the award-winning writer brian k. vaughan (...</td>\n    </tr>\n    <tr>\n      <th>1593</th>\n      <td>legend (legend #1)</td>\n      <td>here there is an alternative title edition for...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Pretrained Model\n\nWe will use the T5 model, a multitask model.  \n**T5 (Text-To-Text Transfer Transformer)** is a model developed by Google that was trained to handle many NLP tasks using a unified format. Every task is treated as text-to-text. So instead of having separate models for translation, summarization, classification, etc., T5 uses task-specific prefix tokens to understand what it's supposed to do.  \n  \nExamples:  \n- \"translate English to German: That is good\" → \"Das ist gut\"\n- \"summarize: This book is about...\" → \"Short summary\"\n- \"cola sentence: The cat sat on the mat\" → \"acceptable\" (grammatical acceptability task)\n\nIn our case - generate a book title based on a book description - title is usually a short, high-level summary. So we're treating title generation as a form of summarization.","metadata":{}},{"cell_type":"code","source":"pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:06:43.239319Z","iopub.execute_input":"2025-07-20T08:06:43.239777Z","iopub.status.idle":"2025-07-20T08:06:49.129031Z","shell.execute_reply.started":"2025-07-20T08:06:43.239757Z","shell.execute_reply":"2025-07-20T08:06:49.128223Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=044ce0d6e1eb9c9142e519c2c20327c76724796565d41f8d1e9ed269bb675672\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:06:49.130473Z","iopub.execute_input":"2025-07-20T08:06:49.130683Z","iopub.status.idle":"2025-07-20T08:06:53.066984Z","shell.execute_reply.started":"2025-07-20T08:06:49.130663Z","shell.execute_reply":"2025-07-20T08:06:53.066216Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import random\nimport torch\nfrom transformers import (\n    T5Tokenizer, T5ForConditionalGeneration,\n    Trainer, TrainingArguments, DataCollatorForSeq2Seq\n)\nfrom datasets import Dataset\nfrom evaluate import load","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:06:53.068202Z","iopub.execute_input":"2025-07-20T08:06:53.068415Z","iopub.status.idle":"2025-07-20T08:07:23.444033Z","shell.execute_reply.started":"2025-07-20T08:06:53.068390Z","shell.execute_reply":"2025-07-20T08:07:23.443218Z"}},"outputs":[{"name":"stderr","text":"2025-07-20 08:07:09.713076: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752998829.901583      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752998829.954558      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# For reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:36.334902Z","iopub.execute_input":"2025-07-20T08:07:36.335542Z","iopub.status.idle":"2025-07-20T08:07:36.346607Z","shell.execute_reply.started":"2025-07-20T08:07:36.335516Z","shell.execute_reply":"2025-07-20T08:07:36.345629Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c05aaa6a8b0>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"The following code prepares our dataset of book descriptions and titles to fine-tune a pretrained **T5 model** so it can generate book titles based on descriptions.  \n  \nThe original DataFrames contain two columns: \"Description\" and \"Title\". These are renamed to \"input_text\" and \"target_text\" to match the expected format for sequence-to-sequence training.  \nA task-specific prefix (\"summarize: \") is added to each input text so the T5 model knows what to do.","metadata":{}},{"cell_type":"code","source":"# Convert to the appropriate format\ndf_train = df_train.rename(columns={'Description': 'input_text', 'Title': 'target_text'})\ndf_train['input_text'] = \"summarize: \" + df_train['input_text']  # T5 uses task-specific prefix tokens\n\ndf_valid = df_valid.rename(columns={'Valid_Description': 'input_text', 'Valid_Title': 'target_text'})\ndf_valid['input_text'] = \"summarize: \" + df_valid['input_text'] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:37.159317Z","iopub.execute_input":"2025-07-20T08:07:37.159692Z","iopub.status.idle":"2025-07-20T08:07:37.173313Z","shell.execute_reply.started":"2025-07-20T08:07:37.159666Z","shell.execute_reply":"2025-07-20T08:07:37.172504Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"The pandas DataFrame is converted into a **Huggingface Dataset**, which integrates better with the Huggingface Trainer and transformers library.","metadata":{}},{"cell_type":"code","source":"# Convert pandas DataFrames to Hugging Face Datasets\ndataset_train = Dataset.from_pandas(df_train)\ndataset_valid = Dataset.from_pandas(df_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:37.696254Z","iopub.execute_input":"2025-07-20T08:07:37.696586Z","iopub.status.idle":"2025-07-20T08:07:37.738338Z","shell.execute_reply.started":"2025-07-20T08:07:37.696560Z","shell.execute_reply":"2025-07-20T08:07:37.737597Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"The **T5Tokenizer** and **T5ForConditionalGeneration model** are loaded from the **Huggingface Model Hub**. \"t5-small\" is a smaller, faster variant of the T5 model — good for low-resource environments.","metadata":{}},{"cell_type":"code","source":"# Load model and tokenizer\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:38.461681Z","iopub.execute_input":"2025-07-20T08:07:38.461982Z","iopub.status.idle":"2025-07-20T08:07:41.349600Z","shell.execute_reply.started":"2025-07-20T08:07:38.461959Z","shell.execute_reply":"2025-07-20T08:07:41.349088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73c3aedcdeb423c886ed8496b11bba9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87ba531cc87942a9be0dafa3c5782a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ce6b24ff98460791d69970a3bc8737"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86dcfaafa57245cb99c9edb09ce1101e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0098b9cbb4db4a03bc369b0d86b778bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389681564d994b0cadc3888838af13de"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"The following function tokenize() prepares the text data (both inputs and targets) for training the model T5. It handles the tokenization of both inputs and targets by using the T5 tokenizer to convert text into token IDs that the model can understand.\n   \nNormally, when training a sequence-to-sequence model like T5, we need to manually pad all inputs and outputs to a fixed max_length and replace padding tokens in labels with -100, so the loss function ignores them. But we will use a DataCollatorForSeq2Seq, that will handle this for us. So this is a simplified tokenization function.","metadata":{}},{"cell_type":"code","source":"# Tokenization function to prepare input and target text\ndef tokenize(batch):\n    # Tokenize the input text (book descriptions) with truncation\n    model_input = tokenizer(batch['input_text'], truncation=True)\n    \n    # Switch tokenizer to \"target mode\" for encoding the output text (book titles)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(batch['target_text'], truncation=True)\n    \n    # Assign tokenized labels to the 'labels' key\n    model_input[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:46.062801Z","iopub.execute_input":"2025-07-20T08:07:46.063566Z","iopub.status.idle":"2025-07-20T08:07:46.067519Z","shell.execute_reply.started":"2025-07-20T08:07:46.063541Z","shell.execute_reply":"2025-07-20T08:07:46.066892Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"We apply the tokenization function to the full datasets (dataset_train and dataset_valid) using the .map() method. The batched=True argument enables batch-wise processing for better performance.","metadata":{}},{"cell_type":"code","source":"# Apply tokenization to training and validation datasets\ntokenized_train = dataset_train.map(tokenize, batched=True)\ntokenized_valid = dataset_valid.map(tokenize, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:47.150803Z","iopub.execute_input":"2025-07-20T08:07:47.151515Z","iopub.status.idle":"2025-07-20T08:07:49.716124Z","shell.execute_reply.started":"2025-07-20T08:07:47.151493Z","shell.execute_reply":"2025-07-20T08:07:49.715432Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e6e3193ed8744eb8a0ae6eb89d614da"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3959: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4e4d9319e5541fc8e48f68997ae8ab0"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"A **DataCollatorForSeq2Seq** is created to handle dynamic padding during training and evaluation. Using a collator allows the model to pad only to the length of the longest sequence in each batch during training — improving efficiency.\n    \nIt dynamically pads each batch to the maximum length in that batch — more memory-efficient than static padding.  \nIt automatically replaces padding tokens in labels with -100.  \nIt works seamlessly with a Trainer.","metadata":{}},{"cell_type":"code","source":"# Create a data collator that dynamically pads inputs and labels per batch and ensures padding tokens in labels are masked out during loss computation\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:51.449212Z","iopub.execute_input":"2025-07-20T08:07:51.449716Z","iopub.status.idle":"2025-07-20T08:07:51.453166Z","shell.execute_reply.started":"2025-07-20T08:07:51.449683Z","shell.execute_reply":"2025-07-20T08:07:51.452568Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"The next function compute_metrics() is used during evaluation of the model. It computes the **ROUGE-1** score, that we have seen in Part2 of the project, between the model’s predicted text and the reference labels.\n\n1. Input Handling  \nThe function expects a tuple (predictions, labels) as input.\nIf predictions contains extra information (logits plus scores), it selects only the token IDs or logits from the first element.\n\n3. Format Normalization  \nIf the model output is still in logits (not token IDs), it applies argmax to pick the most probable token at each position.\nIf predictions are beam search outputs (shape: batch_size × num_beams × seq_len), only the first beam (most likely hypothesis) is selected.\n\n\n5. Label Preparation  \nLabels used during training typically use -100 to mask out padding tokens.\nBefore decoding them back into text, these are replaced with the tokenizer's pad_token_id so they can be correctly transformed into strings.\n\n6. Decoding  \nBoth predictions and labels (now just token IDs) are converted back to readable strings using tokenizer.batch_decode(), skipping special tokens like pad and eos.\n\n7. Metric Calculation  \nThe decoded predictions and labels are passed to the ROUGE metric. WIth use_stemmer=True the process of reducing words to their base or root form is avtive. For example \"running\" becomes \"run\" and \"cats\" becomes \"cat\"  \nROUGE-1 is commonly used for evaluating short text generation tasks. It measures the overlap of unigrams (individual words) between the predicted and reference texts, making it well-suited for short, concise outputs.  \nThe function extracts the ROUGE-1 F1 score, multiplies it by 100 (to express it as a percentage) and returns it.\n","metadata":{}},{"cell_type":"code","source":"# Load the ROUGE metric using the datasets library\nrouge = load(\"rouge\")\n\n# Define a function to compute evaluation metrics\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n\n    # If predictions is a tuple, take only the first part\n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n\n    # If predictions are raw logits (3D: batch_size x seq_len x vocab_size), apply argmax to get token IDs\n    if predictions.ndim == 3 and predictions.shape[-1] == tokenizer.vocab_size:\n        predictions = np.argmax(predictions, axis=-1)\n\n    # If predictions have shape (batch_size, num_beams, seq_len), take only the first beam\n    if predictions.ndim == 3:\n        predictions = predictions[:, 0, :]\n\n    # Replace -100 in labels with the tokenizer’s pad_token_id (so we can decode them correctly)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n    # Ensure prediction IDs are within the valid vocabulary range\n    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n\n    # Decode predicted and reference sequences to strings\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Compute the ROUGE scores (with stemming)\n    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    \n    # Extract the ROUGE-1 F1 score and scale to percentage\n    rouge_1 = result[\"rouge1\"] * 100\n    \n    return {\"rouge1\": round(rouge_1, 2)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:54.897512Z","iopub.execute_input":"2025-07-20T08:07:54.897797Z","iopub.status.idle":"2025-07-20T08:07:55.786563Z","shell.execute_reply.started":"2025-07-20T08:07:54.897775Z","shell.execute_reply":"2025-07-20T08:07:55.785707Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d88126b46524252a3d7cbb42c506893"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"The following code sets up a Huggingface Trainer to fine-tune the model.  \n  \nIt defines **TrainingArguments**:  \n- Small batch size (batch_size) allows training even on low-memory machines like laptops or free GPUs.  \n- Logging step (logging_steps) gives detailed feedback on what’s happening at each training step.  \n- No external logging (report_to=\"none\") ensures clean local runs without integration into tracking tools like TensorBoard or W&B.  \n- TQDM progress bar is enabled, which provides a live, visual indicator of training progress in the console.  ","metadata":{}},{"cell_type":"code","source":"# Define TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir='./results',                  # Directory where model checkpoints will be saved\n    num_train_epochs=10,                     # Number of full passes through the training dataset. 10 epochs are enough because the dataset is small and we want to prevent overfitting.\n    per_device_train_batch_size=10,          # Number of training examples per GPU/CPU. per_device_train_batch_size=10, keeps memory use low. \n    do_eval=True,                            # Enable evaluation on the validation set at the end of training\n    logging_dir='./logs',                    # Directory to store log files\n    logging_steps=25,                        # Log metrics\n    save_strategy=\"epoch\",                   # Save a checkpoint\n    report_to=\"none\",                        # Disable integration with tools like WandB or TensorBoard\n    disable_tqdm=False                       # Show the training progress bar\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:55.788148Z","iopub.execute_input":"2025-07-20T08:07:55.788926Z","iopub.status.idle":"2025-07-20T08:07:55.816188Z","shell.execute_reply.started":"2025-07-20T08:07:55.788895Z","shell.execute_reply":"2025-07-20T08:07:55.815642Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"We initialize the **Trainer** object from Hugging Face that manages the full training and evaluation pipeline.","metadata":{}},{"cell_type":"code","source":"# Initialize Trainer\ntrainer = Trainer(\n    model=model,                             \n    args=training_args,                      # The training configuration defined above\n    train_dataset=tokenized_train,           # The tokenized training dataset\n    eval_dataset=tokenized_valid,            # The tokenized validation dataset\n    data_collator=data_collator,             # The DataCollator defined above\n    tokenizer=tokenizer,                     # Tokenizer used to process text data\n    compute_metrics=compute_metrics          # Function to compute evaluation metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:07:56.466019Z","iopub.execute_input":"2025-07-20T08:07:56.466277Z","iopub.status.idle":"2025-07-20T08:07:56.728495Z","shell.execute_reply.started":"2025-07-20T08:07:56.466261Z","shell.execute_reply":"2025-07-20T08:07:56.727933Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2088117698.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Let's train the model.","metadata":{}},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:08:02.483586Z","iopub.execute_input":"2025-07-20T08:08:02.484341Z","iopub.status.idle":"2025-07-20T08:13:19.439145Z","shell.execute_reply.started":"2025-07-20T08:08:02.484314Z","shell.execute_reply":"2025-07-20T08:13:19.438515Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1600/1600 05:15, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>4.296400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.591800</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>3.139900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.943800</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>2.929100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.951400</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>2.738700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.721500</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>2.757200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.615900</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>2.561300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.456800</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>2.691500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.397600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>2.528600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.456800</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>2.630900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.488700</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>2.318800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.295900</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>2.324400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>2.369000</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>2.355700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.373500</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>2.335100</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>2.186500</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>2.194000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.371100</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>2.204800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>2.263800</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>2.205500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.215300</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>2.138100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>2.247600</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>2.175000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.232700</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>2.134400</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>2.070800</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>2.112700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.128500</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>2.060700</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>2.200200</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>2.092200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.034700</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>2.072700</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>2.087100</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>2.038000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.142500</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>2.040400</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>2.002200</td>\n    </tr>\n    <tr>\n      <td>1275</td>\n      <td>2.033000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.056400</td>\n    </tr>\n    <tr>\n      <td>1325</td>\n      <td>2.054400</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>2.007500</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>2.007200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.023800</td>\n    </tr>\n    <tr>\n      <td>1425</td>\n      <td>2.021700</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>2.013200</td>\n    </tr>\n    <tr>\n      <td>1475</td>\n      <td>2.098900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.013300</td>\n    </tr>\n    <tr>\n      <td>1525</td>\n      <td>2.043100</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>2.048600</td>\n    </tr>\n    <tr>\n      <td>1575</td>\n      <td>1.989500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.963800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1600, training_loss=2.3327521181106565, metrics={'train_runtime': 316.5165, 'train_samples_per_second': 50.361, 'train_steps_per_second': 5.055, 'total_flos': 2072905076244480.0, 'train_loss': 2.3327521181106565, 'epoch': 10.0})"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"The loss decreases consistently, but the rate of improvement is slowing in later steps.","metadata":{}},{"cell_type":"markdown","source":"We evaluate the predictions using **trainer.evaluate()** to run the model on the validation dataset specified earlier in the Trainer.  \nIt computes the loss and the ROUGE-1 metric by using the compute_metrics function we passed during Trainer initialization to generate evaluation scores.","metadata":{}},{"cell_type":"code","source":"# Evaluation\nfinal_metrics = trainer.evaluate()\nprint(\"Final Evaluation Metrics:\", final_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:13:43.786035Z","iopub.execute_input":"2025-07-20T08:13:43.786333Z","iopub.status.idle":"2025-07-20T08:16:07.262637Z","shell.execute_reply.started":"2025-07-20T08:13:43.786311Z","shell.execute_reply":"2025-07-20T08:16:07.261712Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Evaluation Metrics: {'eval_loss': 2.1549863815307617, 'eval_rouge1': 0.0, 'eval_runtime': 143.358, 'eval_samples_per_second': 0.698, 'eval_steps_per_second': 0.091, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Some explanation:\n  \n**eval_loss:**  \nThis is the average loss value computed on the validation dataset.  \nThe loss is moderate, but not necessarily a good indicator of real performance in a text generation task. On its own, it doesn’t reveal whether the model generates useful titles.  \n**eval_rouge1:**  \nA ROUGE-1 score of 0.0 means the model’s generated output has no significant overlap with the true titles. This is a strong signal that the model is not learning to generate meaningful or accurate titles.  \n**eval_runtime:**  \nTotal time spent evaluating the model on the validation dataset.  \n**eval_samples_per_second:**  \nThe speed at which the model processes samples during evaluation.  \n**eval_steps_per_second:**  \nSimilar to above, this refers to how fast the evaluation runs per step (batch).  \n**epoch:**  \nIndicates that the results are from the last epoch of training.","metadata":{}},{"cell_type":"markdown","source":"Now let's see how the pretrained model performs on the test data.\n  \nThe next function, generate_title, takes a book description as input and generates a predicted book title using our pretrained T5 model.\n  \nIt prepends \"summarize: \" to the description, to specify the summarization task.  \nThe input text is tokenized and converted into PyTorch tensors (return_tensors=\"pt\"). It is truncated if too long to fit within the model’s maximum token limit (512 tokens for T5-small).  \nThe model’s generate method produces a sequence of output token IDs representing the predicted title, limited to a maximum length of 32 tokens.  \nFinally, the generated token IDs are decoded back into a readable string, omitting any special tokens like padding or start/end markers. ","metadata":{}},{"cell_type":"code","source":"# Function to generate a title from a given description\ndef generate_title(description_text):\n    # Prepend the T5 task prefix \"summarize:\" to the input text\n    input_text = \"summarize: \" + description_text\n    \n    # Tokenize the input and convert it to a PyTorch tensor\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    \n    # Move the input tensor to the same device as the model (here GPU)\n    input_ids = input_ids.to(model.device)\n    \n    # Generate output token IDs using beam search decoding\n    output_ids = model.generate(input_ids, max_length=32, num_beams=3, early_stopping=True)\n    \n    # Decode the token IDs into a string and remove special tokens\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:16:52.620109Z","iopub.execute_input":"2025-07-20T08:16:52.620437Z","iopub.status.idle":"2025-07-20T08:16:52.625131Z","shell.execute_reply.started":"2025-07-20T08:16:52.620415Z","shell.execute_reply":"2025-07-20T08:16:52.624512Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"We loop through the first 3 entries in the test dataset by using the generate_title() defined above:  \nFor each row, it retrieves the original description and the true title. It then calls the model to generate a title based on the description. Finally, it prints the original description, the original title and the generated title for comparison.","metadata":{}},{"cell_type":"code","source":"# Loop through the first 3 rows of df_test and generate titles\nfor i in range(3):\n    # Get the original description and corresponding title from the test dataframe\n    original_description = df_test.loc[i, 'Test_Description']\n    original_title = df_test.loc[i, 'Test_Title']  \n    \n    # Generate a title using the model based on the description\n    generated_title = generate_title(description_text=original_description)\n    \n    # Print the original and generated content for comparison\n    print(f\"Original Description #{i+1}:\\n{original_description}\\n\")\n    print(f\"Original Title #{i+1}: {original_title}\\n\")\n    print(f\"Generated Title #{i+1}: {generated_title}\\n\")\n    print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:16:53.663732Z","iopub.execute_input":"2025-07-20T08:16:53.664330Z","iopub.status.idle":"2025-07-20T08:16:54.415106Z","shell.execute_reply.started":"2025-07-20T08:16:53.664305Z","shell.execute_reply":"2025-07-20T08:16:54.414497Z"}},"outputs":[{"name":"stdout","text":"Original Description #1:\nstarting over sucks.when we moved to west virginia right before my senior year, i'd pretty much resigned myself to thick accents, dodgy internet access, and a whole lot of boring… until i spotted my hot neighbor, with his looming height and eerie green eyes. things were looking up.and then he opened his mouth.daemon is infuriating. arrogant. stab-worthy. we do not get alon starting over sucks.when we moved to west virginia right before my senior year, i'd pretty much resigned myself to thick accents, dodgy internet access, and a whole lot of boring… until i spotted my hot neighbor, with his looming height and eerie green eyes. things were looking up.and then he opened his mouth.daemon is infuriating. arrogant. stab-worthy. we do not get along. at all. but when a stranger attacks me and daemon literally freezes time with a wave of his hand, well, something… unexpected happens. the hot alien living next door marks me.you heard me. alien. turns out daemon and his sister have a galaxy of enemies wanting to steal their abilities, and daemon's touch has me lit up like the vegas strip. the only way i'm getting out of this alive is by sticking close to daemon until my alien mojo fades. if i don't kill him first, that is.\n\nOriginal Title #1: obsidian (lux #1)\n\nGenerated Title #1: if i don't kill him first, that's what i'm getting out of this alive\n\n--------------------------------------------------------------------------------\nOriginal Description #2:\nthe age of genius explores the eventful intertwining of outward event and inner intellectual life to tell, in all its richness and depth, the story of the 17th century in europe. it was a time of creativity unparalleled in history before or since, from science to the arts, from philosophy to politics. acclaimed philosopher and historian a.c. grayling points to three primar the age of genius explores the eventful intertwining of outward event and inner intellectual life to tell, in all its richness and depth, the story of the 17th century in europe. it was a time of creativity unparalleled in history before or since, from science to the arts, from philosophy to politics. acclaimed philosopher and historian a.c. grayling points to three primary factors that led to the rise of vernacular (popular) languages in philosophy, theology, science, and literature; the rise of the individual as a general and not merely an aristocratic type; and the invention and application of instruments and measurement in the study of the natural world.grayling vividly reconstructs this unprecedented era and breathes new life into the major figures of the seventeenth century intelligentsia who span literature, music, science, art, and philosophy--shakespeare, monteverdi, galileo, rembrandt, locke, newton, descartes, vermeer, hobbes, milton, and cervantes, among many more. during this century, a fundamentally new way of perceiving the world emerged as reason rose to prominence over tradition, and the rights of the individual took center stage in philosophy and politics, a paradigmatic shift that would define western thought for centuries to come.\n\nOriginal Title #2: the age of genius: the seventeenth century and the birth of the modern mind\n\nGenerated Title #2: the age of genius\n\n--------------------------------------------------------------------------------\nOriginal Description #3:\ndespite the tumor-shrinking medical miracle that has bought her a few years, hazel has never been anything but terminal, her final chapter inscribed upon diagnosis. but when a gorgeous plot twist named augustus waters suddenly appears at cancer kid support group, hazel's story is about to be completely rewritten.insightful, bold, irreverent, and raw, the fault in our stars despite the tumor-shrinking medical miracle that has bought her a few years, hazel has never been anything but terminal, her final chapter inscribed upon diagnosis. but when a gorgeous plot twist named augustus waters suddenly appears at cancer kid support group, hazel's story is about to be completely rewritten.insightful, bold, irreverent, and raw, the fault in our stars is award-winning author john green's most ambitious and heartbreaking work yet, brilliantly exploring the funny, thrilling, and tragic business of being alive and in love.\n\nOriginal Title #3: the fault in our stars\n\nGenerated Title #3: the fault in our stars\n\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"Example 1  \nGenerated Title: if i don't kill him first, that's what i'm getting out of this alive  \nThe generated title captures the tone but not the theme (sci-fi/romance). Not really a suitable title. The model used here (T5) was originally trained with a \"summarize\" prefix for summarization tasks, not specifically for title generation. Since there is no native \"title generation\" prefix in the T5 pretraining objectives, we have reused the summarization objective by prepending \"summarize:\" to the input. While this workaround is reasonable, it may not align perfectly with the expectations of concise title generation and may encourage the model to produce longer outputs rather than short, catchy titles.  \n  \nExample 2  \nGenerated Title: The Age of Genius  \nThe generated title is a shortened but accurate and acceptable version of the original title.  \n  \nExample 3  \nGenerated Title: The Fault in Our Stars  \nThe generated title i a perfect match. Likely memorized, but still ideal. T5 was pretrained on a massive public corpus (books, Wikipedia, web data,...). Since The Fault in Our Stars is a very famous book, it’s almost certain that both its description and title were present in the pretraining data. The model might have memorized this mapping during pretraining, even before we fine-tuned it. So during testing, when it saw the description, it simply retrieved the known title from its pretraining “memory.”  \n  \nOverall, the model seems to perform well for nonfiction and known titles but struggles with fiction when the title is abstract or creative.  ","metadata":{}}]}