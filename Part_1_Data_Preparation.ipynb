{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Introduction  \nThis project focuses on building and evaluating models for automatic book title generation based on book descriptions. It is divided into three main parts — from raw text extraction and preparation to model training, fine-tuning and evaluation.\n  \nGenerating meaningful and creative titles from descriptions is a particularly challenging task in natural language processing. Unlike summaries or classifications, titles are often abstract, metaphorical or intentionally ambiguous and they don’t always offer a direct reflection of the book’s content. While descriptions provide clear insight into the storyline or subject matter, titles may be poetic, symbolic or chosen for marketing appeal, making them hard to predict from a purely semantic or structural standpoint.\n  \nIn addition to the inherent difficulty of the task, this project is further constrained by the limited size of the dataset. A small corpus increases the risk of overfitting and reduces the model’s ability to generalize. To counter this, careful preprocessing and data augmentation techniques are applied to enhance the dataset and improve training outcomes.\n  \nBy combining practical data handling with advanced transformer-based models, the project explores how far we can push automated title generation under real-world constraints.\n  \n### Part 1: Data Collection and Preparation  \nIn the first stage, book data is collected through web scraping from an online bookstore (books.toscrape.com is a demo website for web scraping purposes). This includes titles, descriptions, star ratings and prices. Since the raw dataset is relatively small, two data augmentation techniques are applied to improve model generalization:  \nBacktranslation: English descriptions are translated into German and then back into English to generate semantically equivalent but syntactically varied text.  \nParaphrasing: A T5-based paraphraser generates alternative versions of descriptions while preserving the original meaning.  \nThis step ensures that the training data is diverse and suitable for fine-tuning language models.  \n  \n### Part 2: Transformer Model for Title Generation  \nThe second part involves building a custom transformer-based model that generates book titles from descriptions. This includes:  \nByte-Pair Encoding (BPE) for effective tokenization of input/output text.  \nHyperparameter optimization using Optuna, guided by the ROUGE score as the evaluation metric.  \nThe goal is to learn an end-to-end mapping from description to title.\n  \n### Part 3: Fine-Tuning a Pretrained T5 Model  \nIn the final part, a pretrained T5 model is fine-tuned on the same description-to-title task. By leveraging the model's pretrained language understanding, we aim to improve performance on title generation, especially with limited training data. This step serves as a benchmark against the custom transformer developed in Part 2.","metadata":{}},{"cell_type":"markdown","source":"# Transformer-Based Title Generation - Part 1","metadata":{}},{"cell_type":"markdown","source":"The first part of the project focuses on extracting and preparing data for the title generation task. Book information — such as titles, descriptions, ratings and prices—is collected via web scraping from an online bookstore. The primary objective in this stage is to ensure that the input data is clean, consistent and suitable for training language models.\n  \nAlthough the preprocessing applied is minimal, it is intentionally designed to be efficient and effective. The code removes only the most disruptive forms of noise while preserving the natural linguistic structure of the text. This design choice ensures that the Byte-Pair Encoding (BPE) tokenizer (used in part 2 of the project) can learn consistent subword patterns, which is critical for generating fluent and accurate book titles. Subword-level regularity is particularly important in this task, as small variations in the input can significantly affect the model's output.\n  \nTo further improve data quality, non-English book descriptions are filtered out using language detection. This step ensures that the model is trained exclusively on English-language text, avoiding confusion caused by multilingual input and improving model performance on the downstream task.\n  \nBecause the scraped dataset is relatively small, two complementary data augmentation techniques are applied to expand and diversify the training data:  \nBacktranslation: Book descriptions are translated from English to German and then back to English. This produces alternate phrasings that preserve meaning but introduce syntactic variety.  \nParaphrasing: A pretrained T5 paraphraser is used to generate semantically equivalent but lexically diverse versions of each description.  \n  \nBy combining careful filtering, lightweight preprocessing and powerful augmentation strategies, Part 1 sets a foundation for training models in the following stages of the project.","metadata":{}},{"cell_type":"markdown","source":"## Web Scraping Book Data from books.toscrape.com\n\nThe following script scrapes information about books listed on the website books.toscrape.com. It collects the title, star rating, price and description of each book across multiple pages, then stores the data in a pandas DataFrame.  \nNote: This website is a publicly available demo created specifically for web scraping practice. It does not list real-world books — all titles, prices and descriptions are fictional and intended for educational or testing purposes only.","metadata":{}},{"cell_type":"code","source":"pip install langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:25:07.858228Z","iopub.execute_input":"2025-07-16T13:25:07.858948Z","iopub.status.idle":"2025-07-16T13:25:15.428357Z","shell.execute_reply.started":"2025-07-16T13:25:07.858923Z","shell.execute_reply":"2025-07-16T13:25:15.427461Z"}},"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=ad9d03aff94a9773d1dde618d56770cd340dbd4c29fb7c28bbd7a9c5eed1294e\n  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langdetect import detect, DetectorFactory\nimport re\nimport html\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:25:18.535996Z","iopub.execute_input":"2025-07-16T13:25:18.536316Z","iopub.status.idle":"2025-07-16T13:25:23.767338Z","shell.execute_reply.started":"2025-07-16T13:25:18.536288Z","shell.execute_reply":"2025-07-16T13:25:23.766716Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"books = []\n\n# Base URL to resolve relative links\nbase_url = \"https://books.toscrape.com/catalogue/\"\n\n# Load the first page\nresponse = requests.get(f\"{base_url}page-1.html\")\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Read pagination info to determine the number of pages\npager = soup.find('ul', class_='pager')\nif pager:\n    current_page_text = pager.find('li', class_='current').text.strip()\n    last_page = int(current_page_text.split()[-1])  # Extract the \"50\"\nelse:\n    last_page = 1  # If no pagination, assume only one page\n\n# Iterate through all pages\nfor page in range(1, last_page + 1):\n    url = f\"{base_url}page-{page}.html\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # The book listings are inside an <ol> tag\n    ol = soup.find('ol')   # Go into soup and find all ol. ol=ordered list. Contains all articles=books\n    articles = ol.find_all('article', class_='product_pod')  # Go into ol and find all articles=books\n\n    for article in articles:\n        # Extract title from the image alt attribute\n        image = article.find('img')\n        title = image.attrs['alt']   # The full title is here\n\n        # Extract star rating (\"One\", \"Two\", \"Three\", etc.)\n        star_tag = article.find('p')\n        star = star_tag['class'][1]  # We only need the number of stars\n\n        # Extract price and convert to float\n        price = article.find('p', class_='price_color').text\n        price = float(price[1:])  # Remove \"£\". We don't need the currency, therefore [1:]. Convert string to float.\n\n        # Construct full link to the book's detail page\n        link_tag = article.find('h3').find('a')\n        book_url = base_url + link_tag['href']\n\n        # Visit the book's detail page to extract its description\n        book_response = requests.get(book_url)\n        book_soup = BeautifulSoup(book_response.content, 'html.parser')\n\n        # Find the book description in a <meta name=\"description\"> tag\n        description = book_soup.find('meta', attrs={\"name\": \"description\"})\n        if description:\n            description_text = description['content'].strip()\n        else:\n            # Alternatively, try finding the description inside an <article><p> block\n            description_tags = book_soup.find('article').find_all('p')\n            if len(description_tags) > 3:\n                description_text = description_tags[3].text.strip()\n            else:\n                description_text = \"No description found\"\n\n        # Save extracted data\n        books.append([title, star, price, description_text])\n\n# Convert the list of books into a pandas DataFrame\ndf = pd.DataFrame(books, columns=['Title', 'Star Rating', 'Price', 'Description'])\ndf.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:25:23.768359Z","iopub.execute_input":"2025-07-16T13:25:23.768754Z","iopub.status.idle":"2025-07-16T13:28:01.426130Z","shell.execute_reply.started":"2025-07-16T13:25:23.768727Z","shell.execute_reply":"2025-07-16T13:28:01.425400Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                 Title Star Rating  Price  \\\n995  Alice in Wonderland (Alice's Adventures in Won...         One  55.53   \n996   Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)        Four  57.06   \n997  A Spy's Devotion (The Regency Spies of London #1)        Five  16.97   \n998                1st to Die (Women's Murder Club #1)         One  53.98   \n999                 1,000 Places to See Before You Die        Five  26.08   \n\n                                           Description  \n995                                                     \n996  High school student Kei Nagai is struck dead i...  \n997  In England’s Regency era, manners and elegance...  \n998  James Patterson, bestselling author of the Ale...  \n999  Around the World, continent by continent, here...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Star Rating</th>\n      <th>Price</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>995</th>\n      <td>Alice in Wonderland (Alice's Adventures in Won...</td>\n      <td>One</td>\n      <td>55.53</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)</td>\n      <td>Four</td>\n      <td>57.06</td>\n      <td>High school student Kei Nagai is struck dead i...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>A Spy's Devotion (The Regency Spies of London #1)</td>\n      <td>Five</td>\n      <td>16.97</td>\n      <td>In England’s Regency era, manners and elegance...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>1st to Die (Women's Murder Club #1)</td>\n      <td>One</td>\n      <td>53.98</td>\n      <td>James Patterson, bestselling author of the Ale...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>1,000 Places to See Before You Die</td>\n      <td>Five</td>\n      <td>26.08</td>\n      <td>Around the World, continent by continent, here...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"The next code filters out all rows in the DataFrame df where the \"Description\" column contains non-English text. It uses the **langdetect library** to identify the language of each description. Only English entries are desired in our DataFrame, because we will use tasks with pretrained English models.\n  \nThe script also:  \n- Ensures deterministic language detection using a fixed seed.\n- Calculates and prints the number of removed rows.\n- Displays the first non-English description that was removed for inspection (if any).","metadata":{}},{"cell_type":"code","source":"# Ensure consistent results from langdetect\nDetectorFactory.seed = 0\n\n# Function to check if a text is English\ndef is_english(text):\n    try:\n        return detect(text) == 'en'\n    except:\n        return False  # Handle short/empty strings or detection errors\n\n# Store original number of rows\noriginal_len = len(df)\n\n# Identify rows where description is NOT English\nnon_english_mask = ~df['Description'].apply(is_english)\n\n# Save non-English rows before dropping (optional preview)\nnon_english_rows = df[non_english_mask]\n\n# Drop rows with non-English descriptions\ndf = df[~non_english_mask].reset_index(drop=True)\n\n# Print number of removed rows\nremoved_rows = original_len - len(df)\nprint(f\"{removed_rows} rows removed (non-English descriptions).\")\n\n# Show the first removed row (if there was any)\nif not non_english_rows.empty:\n    print(\"\\nFirst removed row:\")\n    print(non_english_rows.iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:28:12.684487Z","iopub.execute_input":"2025-07-16T13:28:12.684752Z","iopub.status.idle":"2025-07-16T13:28:16.813638Z","shell.execute_reply.started":"2025-07-16T13:28:12.684731Z","shell.execute_reply":"2025-07-16T13:28:16.812951Z"}},"outputs":[{"name":"stdout","text":"3 rows removed (non-English descriptions).\n\nFirst removed row:\nTitle                                                 Soumission\nStar Rating                                                  One\nPrice                                                       50.1\nDescription    Dans une France assez proche de la nôtre, un h...\nName: 2, dtype: object\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"The following code defines a preprocessing function that prepares raw text (book titles and descriptions) for use with transformer models that rely on subword tokenization, specifically Byte Pair Encoding (BPE).\n\nSince BPE learns to represent frequently occurring subword units, the goal is to preserve as much meaningful structure and variability in the raw text as possible. This means:  \n- Lowercasing (text.lower()) helps reduce vocabulary size. For example, \"Book\" and \"book\" will be treated as the same token.  \n- Punctuation & special characters are preserved: The function does not remove punctuation, stopwords or diacritics (German umlauts). This is intentional — BPE benefits from learning frequent patterns that may include punctuation and character combinations (\".\", \"-ing\", \"schön\").  \n- HTML, URLs and invisible/control characters are removed. These are noisy and unlikely to form useful subword patterns. Removing them reduces unnecessary tokens and improves learning efficiency.\n- Unicode normalization: Zero-width spaces or RTL/LTR markers can break tokenization and result in inconsistent splits. Their removal ensures that the input text is clean and consistent.  \n- Whitespace normalization: Collapsing multiple spaces helps BPE find more predictable patterns. Excess whitespace doesn't carry semantic meaning.  \n- HTML entity decoding (\"&amp\"; → \"&\"): BPE would treat entities like &amp; as entirely different from &, fragmenting the vocabulary unnecessarily.\n\nFor BPE-based models, removing stopwords or applying stemming/lemmatization would eliminate important patterns (common suffixes like -ing, -ed). It would reduce the richness of training data, break useful co-occurrence statistics that BPE needs for efficient subword learning. Instead, the goal is to retain raw structure while removing noise that would interfere with token splitting.","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    if not isinstance(text, str):\n        raise ValueError(\"Input must be a string\")\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove \"...more\" (and variants like \"... more\" or \"…more\")\n    text = re.sub(r'\\.\\.\\.\\s*more', '', text)\n    text = re.sub(r'…\\s*more', '', text)\n\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n\n    # Remove control characters (ASCII control codes)\n    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n\n    # Remove specific Unicode characters like zero-width space\n    text = re.sub(r'\\u200b', '', text)\n\n    # Remove other invisible formatting characters (RTL/LTR marks)\n    text = re.sub(r'[\\u200e\\u200f\\u202a-\\u202e]', '', text)\n\n    # Collapse multiple spaces into one\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # Normalize punctuation marks\n    text = text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n\n    # Decode HTML entities (&amp; → &)\n    text = html.unescape(text)\n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:28:21.893190Z","iopub.execute_input":"2025-07-16T13:28:21.893471Z","iopub.status.idle":"2025-07-16T13:28:21.899051Z","shell.execute_reply.started":"2025-07-16T13:28:21.893453Z","shell.execute_reply":"2025-07-16T13:28:21.898285Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Apply preprocessing to the \"Title\" and \"Description\" columns\ndf['title_prepro'] = df['Title'].apply(preprocess_text)\ndf['description_prepro'] = df['Description'].apply(preprocess_text)\n\n# Drop unused columns to keep only the preprocessed text\ndf.drop('Title', axis=1, inplace=True)\ndf.drop('Description', axis=1, inplace=True)\ndf.drop('Star Rating', axis=1, inplace=True)\ndf.drop('Price', axis=1, inplace=True)\n\n# Display the cleaned DataFrame\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:28:25.057304Z","iopub.execute_input":"2025-07-16T13:28:25.057831Z","iopub.status.idle":"2025-07-16T13:28:25.225853Z","shell.execute_reply.started":"2025-07-16T13:28:25.057807Z","shell.execute_reply":"2025-07-16T13:28:25.225134Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                            title_prepro  \\\n0                   a light in the attic   \n1                     tipping the velvet   \n2                          sharp objects   \n3  sapiens: a brief history of humankind   \n4                        the requiem red   \n\n                                  description_prepro  \n0  it's hard to imagine a world without a light i...  \n1  \"erotic and absorbing...written with starling ...  \n2  wicked above her hipbone, girl across her hear...  \n3  from a renowned historian comes a groundbreaki...  \n4  patient twenty-nine.a monster roams the halls ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title_prepro</th>\n      <th>description_prepro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a light in the attic</td>\n      <td>it's hard to imagine a world without a light i...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tipping the velvet</td>\n      <td>\"erotic and absorbing...written with starling ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sharp objects</td>\n      <td>wicked above her hipbone, girl across her hear...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sapiens: a brief history of humankind</td>\n      <td>from a renowned historian comes a groundbreaki...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the requiem red</td>\n      <td>patient twenty-nine.a monster roams the halls ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"We will split the data.  \nThis approach ensures that the model is trained on a large portion of the data (train_descriptions, train_titles), validated on a separate subset (valid_descriptions, valid_titles) and finally tested on an unseen subset (test_descriptions, test_titles). This separation helps evaluate the model's performance and generalization ability.  ","metadata":{}},{"cell_type":"code","source":"# Split data\ntitles = df[\"title_prepro\"].astype(str).tolist()\ndescriptions = df[\"description_prepro\"].astype(str).tolist()\n\ntrain_titles, temp_titles, train_descriptions, temp_descriptions = train_test_split(\n    titles, descriptions, test_size=0.2, random_state=42)\nvalid_titles, test_titles, valid_descriptions, test_descriptions = train_test_split(\n    temp_titles, temp_descriptions, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:28:28.397650Z","iopub.execute_input":"2025-07-16T13:28:28.398254Z","iopub.status.idle":"2025-07-16T13:28:28.405051Z","shell.execute_reply.started":"2025-07-16T13:28:28.398214Z","shell.execute_reply":"2025-07-16T13:28:28.404351Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Data Augmentation: Backtranslation\n\n**Backtranslation** is a powerful data augmentation technique used to generate synthetic training data by translating existing text to another language and then translating it back to the original language. This method creates paraphrased versions of our input text while preserving its meaning.\n\nTransformer models like T5 or GPT require large and diverse datasets to generalize well.  \nWe're working with a very small dataset, our model might memorize examples instead of learning general patterns and overfits quickly. It may performs poorly on unseen data. Backtranslation helps increase the diversity of our training examples without requiring labeled data from scratch.\n\nThe purpose of backtranslation in our case is to simulate new ways of describing the same book, not to generate new books or new titles. If we change the target (title), we're no longer augmenting the same example, but rather creating a new training pair — which defeats the point of using backtranslation for data augmentation. In very low-resource scenarios, we might consider augmenting not just the inputs (descriptions), but also the targets (titles). However, this crosses the line from data augmentation into synthetic data generation. Our targets are Book titles that must stay fixed because they are highly specific or creative. Backtranslating can alter meaning, create inaccurate labels or confuse the model. In this case, backtranslation of targets does not preserve label integrity.","metadata":{}},{"cell_type":"code","source":"pip install nlpaug transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:28:31.819994Z","iopub.execute_input":"2025-07-16T13:28:31.820581Z","iopub.status.idle":"2025-07-16T13:28:35.352758Z","shell.execute_reply.started":"2025-07-16T13:28:31.820533Z","shell.execute_reply":"2025-07-16T13:28:35.351958Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.3)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import nlpaug.augmenter.sentence as nas\nimport random\nfrom transformers import MarianMTModel, MarianTokenizer\nimport torch\nfrom nltk import sent_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom tqdm import tqdm\n\n# For reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:28:35.354231Z","iopub.execute_input":"2025-07-16T13:28:35.354486Z","iopub.status.idle":"2025-07-16T13:29:14.011966Z","shell.execute_reply.started":"2025-07-16T13:28:35.354465Z","shell.execute_reply":"2025-07-16T13:29:14.011414Z"}},"outputs":[{"name":"stderr","text":"2025-07-16 13:28:56.716511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752672537.071455      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752672537.172007      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7d1f1e582af0>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"The following code sets up the environment and loads pre-trained translation models from the **Helsinki-NLP collection on Hugging Face**. These models will be used for our backtranslation.\n  \nThis setup includes:  \n- Checking for GPU availability.\n- Loading English-to-German and German-to-English translation models.\n- Loading the corresponding tokenizers.","metadata":{}},{"cell_type":"code","source":"# Enable GPU support if available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define model names for backtranslation:\n# These are pre-trained translation models from Helsinki-NLP\nen_to_de_model_name = \"Helsinki-NLP/opus-mt-en-de\"  # English to German\nde_to_en_model_name = \"Helsinki-NLP/opus-mt-de-en\"  # German to English\n\n# Load the English-to-German tokenizer and model, then move model to the selected device (GPU/CPU)\nen_to_de_tokenizer = MarianTokenizer.from_pretrained(en_to_de_model_name)\nen_to_de_model = MarianMTModel.from_pretrained(en_to_de_model_name).to(device)\n\n# Load the German-to-English tokenizer and model, also moving to the device\nde_to_en_tokenizer = MarianTokenizer.from_pretrained(de_to_en_model_name)\nde_to_en_model = MarianMTModel.from_pretrained(de_to_en_model_name).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:29:16.900063Z","iopub.execute_input":"2025-07-16T13:29:16.900665Z","iopub.status.idle":"2025-07-16T13:29:24.694833Z","shell.execute_reply.started":"2025-07-16T13:29:16.900640Z","shell.execute_reply":"2025-07-16T13:29:24.694274Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40e47a5e9f674c5ab63a8c0d10bce419"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"092eb3588ef84e8097a0eb96f807fedf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a75df6f271b4f3190c847a5e0b546e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7704843381342e9b4bf0b274714ce1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f89bdd22f6894c209e57de8a88b3cbc7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9ede793aad474aa1ea80ad58636e6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6fdad36918040c3abb384c6a7b33ae9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/298M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c3eef522fcc45db83aa56f0ede0ec35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70ab3954b97487fba81f04e0f1618ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17a436287774b65976815e5aaac3aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29d059e1997c495c92ffa8984ea7d227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966701c83b49403a8924bf87856391c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0d311f9ffa14daea1e1effd5023202d"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14393f3dabf64c0ebd652203dd51306b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb6787836b5440889cbcaec7194f394"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/298M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32fad2d808c547149cb5f24e21304ce2"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"The following code implements a robust backtranslation pipeline, where English text is first translated into German and then back into English. \n  \nTo handle longer texts effectively, the code first divides the input into manageable **chunks** using the chunk_text function, which splits the input by sentences and ensures that each chunk stays within a specified word limit. \n  \nThe core translation logic leverages two machine translation models (English to German and German to English), applying probabilistic decoding techniques to introduce variability in the outputs. In both translation steps, the code uses the .generate() method with the following parameters:  \n- **do_sample=True** enables sampling instead of deterministic decoding, allowing the model to produce more diverse outputs.\n- The **top_k=80** setting restricts the sampling to the top 80 most likely tokens, removing extremely unlikely candidates. Complementing this, **top_p=0.97** enables nucleus sampling, which considers the smallest group of top tokens whose cumulative probability is at least 97%, further balancing diversity and coherence.\n- The **temperature=0.99** slightly increases randomness by flattening the probability distribution, promoting natural-sounding variation without losing too much semantic fidelity.\n- Additionally, **max_length=512** caps the length of generated outputs to avoid excessively long or runaway generations, while **early_stopping=True** ensures that the model halts generation once it believes the sentence is complete. ","metadata":{}},{"cell_type":"code","source":"# Chunking function\ndef chunk_text(text, max_words_per_chunk=100):\n    if not text.strip():\n        return []\n\n    # Split text into individual sentences\n    sentences = sent_tokenize(text)\n    chunks = []\n    current_chunk = []\n    current_word_count = 0\n\n    for sentence in sentences:\n        sentence_word_count = len(sentence.split())\n\n        # If adding this sentence would exceed the max word count per chunk, finalize the current chunk\n        if current_word_count + sentence_word_count > max_words_per_chunk:\n            if current_chunk:\n                chunks.append(' '.join(current_chunk))\n            current_chunk = [sentence]\n            current_word_count = sentence_word_count\n        else:\n            current_chunk.append(sentence)\n            current_word_count += sentence_word_count\n\n    # Append the final chunk if it contains any sentences\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n\n    return chunks\n\n\n# Backtranslation EN to DE to EN with error handling\ndef backtranslate_en_to_de_to_en(text, max_words_per_chunk=100):\n    try:\n        # Break the input into manageable chunks\n        chunks = chunk_text(text, max_words_per_chunk=max_words_per_chunk)\n        backtranslated_chunks = []\n\n        for chunk in chunks:\n            # English to German Translation\n            encoded_en = en_to_de_tokenizer(\n                chunk,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=512  # Prevent the model from processing overly long inputs\n            ).to(device)\n\n            translated_de = en_to_de_model.generate(\n                **encoded_en,\n                do_sample=True,      # Enable sampling instead of greedy decoding for diversity. \n                top_k=10,            # Limit sampling to the top most likely next tokens. Smaller k allows less probable tokens to be chosen, increasing variety.\n                top_p=0.2,           # Nucleus sampling: consider only top tokens with cumulative probability. Lower p includes more unpredictable tokens.\n                temperature=1.9,     # Controls randomness: closer to 1.0 means more randomness, lower = more deterministic. Higher temperature = more randomness, less deterministic output.\n                max_length=512,      # Limit output length to prevent excessively long generations\n                early_stopping=True  # Stop when the model thinks the output is complete\n            )\n\n            # Decode the generated German text from token IDs\n            text_de = en_to_de_tokenizer.decode(\n                translated_de[0],\n                skip_special_tokens=True\n            )\n            \n            # German to English Translation\n            encoded_de = de_to_en_tokenizer(\n                text_de,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=512\n            ).to(device)\n\n            translated_en = de_to_en_model.generate(\n                **encoded_de,\n                do_sample=True,\n                top_k=10,\n                top_p=0.2,\n                temperature=1.9,\n                max_length=512,\n                early_stopping=True\n            )\n\n            backtranslated_text = de_to_en_tokenizer.decode(\n                translated_en[0],\n                skip_special_tokens=True\n            ).strip()\n\n            # Preprocessing new text\n            backtranslated_chunks.append(preprocess_text(backtranslated_text))\n\n        return ' '.join(backtranslated_chunks)\n\n    except Exception as e:\n        print(f\"[Backtranslation Error]: {e}\")\n        return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:29:29.495621Z","iopub.execute_input":"2025-07-16T13:29:29.495928Z","iopub.status.idle":"2025-07-16T13:29:29.505253Z","shell.execute_reply.started":"2025-07-16T13:29:29.495908Z","shell.execute_reply":"2025-07-16T13:29:29.504547Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"In the following code, 50% of the training data is augmented using backtranslation, while the corresponding titles are kept unchanged to maintain label consistency. The selected indices for backtranslation are chosen randomly.\n  \nThe remaining 50% of the data, corresponding to the indices not selected here, is reserved for a separate paraphrasing augmentation step. This ensures that the two augmentation methods — backtranslation and paraphrasing — are applied to distinct, non-overlapping portions of the dataset, helping to increase diversity and reduce redundancy in the augmented training samples.\n  \nFor each selected sample, the description is passed through the backtranslation pipeline to generate a new paraphrased description, which is appended to the list of augmented descriptions. The original title is kept unchanged and appended to the list of titles for the augmented data. This way, the model trains on varied inputs with consistent target outputs.","metadata":{}},{"cell_type":"code","source":"# Backtranslation loop — only for a random 50% of samples\nnum_samples = len(train_descriptions)\nbt_indices = random.sample(range(num_samples), int(0.5 * num_samples))\n\n# Select subset of data to augment\nsampled_descriptions = [train_descriptions[i] for i in bt_indices]\nsampled_titles = [train_titles[i] for i in bt_indices]\n\n# Lists for augmented samples\nbt_descriptions = []\nbt_titles = []\n\n# Backtranslate descriptions only. Keep titles unchanged.\nfor orig_desc, orig_title in tqdm(zip(sampled_descriptions, sampled_titles), total=len(bt_indices), desc=\"Backtranslating\"):\n    new_desc = backtranslate_en_to_de_to_en(orig_desc)\n    bt_descriptions.append(new_desc)\n    bt_titles.append(orig_title)  # unchanged title","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T13:29:33.564299Z","iopub.execute_input":"2025-07-16T13:29:33.564570Z","iopub.status.idle":"2025-07-16T14:08:40.869932Z","shell.execute_reply.started":"2025-07-16T13:29:33.564550Z","shell.execute_reply":"2025-07-16T14:08:40.869313Z"}},"outputs":[{"name":"stderr","text":"Backtranslating: 100%|██████████| 398/398 [39:07<00:00,  5.90s/it]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Let's have a look at some backtranslated descriptions.","metadata":{}},{"cell_type":"code","source":"n = 3\n\nfor i in range(n):\n    idx = bt_indices[i] \n\n    print(f\"\\n--- Example {i+1} ---\")\n    print(f\"Original Title:      {train_titles[idx]}\")\n    print(f\"Backtranstaled Title:     {bt_titles[i]}\")\n    print(f\"\\nOriginal Description:\\n{train_descriptions[idx]}\")\n    print(f\"\\nBacktranstaled Description:\\n{bt_descriptions[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:09:10.830398Z","iopub.execute_input":"2025-07-16T14:09:10.830683Z","iopub.status.idle":"2025-07-16T14:09:10.836137Z","shell.execute_reply.started":"2025-07-16T14:09:10.830664Z","shell.execute_reply":"2025-07-16T14:09:10.835372Z"}},"outputs":[{"name":"stdout","text":"\n--- Example 1 ---\nOriginal Title:      the white queen (the cousins' war #1)\nBacktranstaled Title:     the white queen (the cousins' war #1)\n\nOriginal Description:\nphilippa gregory presents the first of a new series set amid the deadly feuds of england known as the wars of the roses.brother turns on brother to win the ultimate prize, the throne of england, in this dazzling account of the wars of the plantagenets. they are the claimants and kings who ruled england before the tudors, and now philippa gregory brings them to life through philippa gregory presents the first of a new series set amid the deadly feuds of england known as the wars of the roses.brother turns on brother to win the ultimate prize, the throne of england, in this dazzling account of the wars of the plantagenets. they are the claimants and kings who ruled england before the tudors, and now philippa gregory brings them to life through the dramatic and intimate stories of the secret players: the indomitable women, starting with elizabeth woodville, the white queen.the white queen tells the story of a woman of extraordinary beauty and ambition who, catching the eye of the newly crowned boy king, marries him in secret and ascends to royalty. while elizabeth rises to the demands of her exalted position and fights for the success of her family, her two sons become central figures in a mystery that has confounded historians for centuries: the missing princes in the tower of london whose fate is still unknown. from her uniquely qualified perspective, philippa gregory explores this most famous unsolved mystery of english history, informed by impeccable research and framed by her inimitable storytelling skills.with the white queen, philippa gregory brings the artistry and intellect of a master writer and storyteller to a new era in history and begins what is sure to be another bestselling classic series from this beloved author.\n\nBacktranstaled Description:\nphilippa gregory presents the first of a new series in the midst of the deadly feuds of england known as the wars of the roses. brother turns to brother to win the ultimate prize, the throne of england, in this dazzling account of the wars of the plantagenets. they are the claimants and kings who ruled england before the tudors, and now philippa gregory brings them to life by philippa gregory presenting the first of a new series amid the deadly feuds of england known as the wars of the roses.brother turns to brother to win the ultimate prize, the throne of england, in this dazzling account of the wars of the plantagenets. they are the claimants and kings who ruled england before the tudors, and now philippa gregory brings them to life through the dramatic and intimate stories of the secret players: the indomitable women, starting with elizabeth woodville, the white queen.the white queen tells the story of a woman of extraordinary beauty and ambition who, catching the eye of the newly crowned boy king, secretly marries him and ascends to kingship. while elizabeth rises up to the demands of her lofty position and fights for the success of her family, her two sons become central figures in a mystery that has been insecure to historians for centuries: the missing princes in the tower of london, whose fate is still unknown. from their uniquely qualified perspective, philippa gregory explores this most famous unresolved mystery of english history, informed by impeccable research and framed by their inimitable storytelling skills. with the white queen, philippa gregory brings the art and intellect of a master writer and storyteller into a new era in history and begins, which is certain to be another bestseller classic series of this beloved author.\n\n--- Example 2 ---\nOriginal Title:      the high mountains of portugal\nBacktranstaled Title:     the high mountains of portugal\n\nOriginal Description:\nin lisbon in 1904, a young man named tomás discovers an old journal. it hints at the existence of an extraordinary artifact that—if he can find it—would redefine history. traveling in one of europe's earliest automobiles, he sets out in search of this strange treasure.thirty-five years later, a portuguese pathologist devoted to the murder mysteries of agatha christie finds in lisbon in 1904, a young man named tomás discovers an old journal. it hints at the existence of an extraordinary artifact that—if he can find it—would redefine history. traveling in one of europe's earliest automobiles, he sets out in search of this strange treasure.thirty-five years later, a portuguese pathologist devoted to the murder mysteries of agatha christie finds himself at the center of a mystery of his own and drawn into the consequences of tomás's quest.fifty years on, a canadian senator takes refuge in his ancestral village in northern portugal, grieving the loss of his beloved wife. but he arrives with an unusual companion: a chimpanzee. and there the century-old quest will come to an unexpected conclusion.the high mountains of portugal—part quest, part ghost story, part contemporary fable—offers a haunting exploration of great love and great loss. filled with tenderness, humor, and endless surprise, it takes the reader on a road trip through portugal in the last century—and through the human soul.\n\nBacktranstaled Description:\nin lisbon in 1904, a young tomás discovers an old journal. it points to the existence of an extraordinary artifact that – if he can find it – the story would be redefined. on the road in one of the earliest automobiles in europe, he makes a search for this strange treasure. thirty-five years later, a portuguese pathologist, dedicated to the murder secrets of the agatha christie, discovers a young tomás-called journal in lisbon in 1904. it points to the existence of an extraordinary artifact that – if he can find it – would redefine the story. in one of europe's earliest automobiles, he travels in search of this strange treasure.thirty-five years later, a portuguese pathologist dedicated to the mystery of agatha christie's murder finds himself at the heart of his own mystery and draws himself into the aftermath of tomás' quest.fifty years later, a canadian senator flees to his ancestral village in northern portugal and mourns the loss of his beloved wife.but he arrives with an unusual companion: a chimpanzee. and there the centuries-old search will come to an unexpected end. the high mountains of portugal – part of the search, part of the ghost history, part of the contemporary fable – offers a penetrating exploration of great love and great losses. filled with tenderness, humor and endless surprise, it takes the reader on a road trip through portugal in the last century – and through the human soul.\n\n--- Example 3 ---\nOriginal Title:      counted with the stars (out from egypt #1)\nBacktranstaled Title:     counted with the stars (out from egypt #1)\n\nOriginal Description:\na story of love, desperation, and hope during a great biblical epochsold into slavery by her father and forsaken by the man she was supposed to marry, young egyptian kiya must serve a mistress who takes pleasure in her humiliation. when terrifying plagues strike egypt, kiya is in the middle of it all.to save her older brother and escape the bonds of slavery, kiya flees wit a story of love, desperation, and hope during a great biblical epochsold into slavery by her father and forsaken by the man she was supposed to marry, young egyptian kiya must serve a mistress who takes pleasure in her humiliation. when terrifying plagues strike egypt, kiya is in the middle of it all.to save her older brother and escape the bonds of slavery, kiya flees with the hebrews during the great exodus. she finds herself utterly dependent on a fearsome god she's only just beginning to learn about, and in love with a man who despises her people. with everything she's ever known swept away, will kiya turn back toward egypt or surrender her life and her future to yahweh?\n\nBacktranstaled Description:\na story of love, despair and hope during a great biblical epoch in slavery from her father and abandoned by the man she was to marry, young egyptian kiya must serve a lover who takes pleasure in her humiliation. when terrifying plagues strike egypt, kiya is in the middle of everything. to save her older brother and escape the bonds of slavery, kiya flees with a story of love, despair and hope during a great biblical epoch into slavery by her father and abandoned by the man she was to marry, young egyptian kiya must serve a lover who takes pleasure in her humiliation. when terrifying plagues strike egypt, kiya is in the middle of everything. to save her older brother and escape the bonds of slavery, kiya flees with the heifers during the great exodus. she finds herself totally dependent on a fear - inspiring god, whom she is just beginning to learn, and in love for a man who despises her people.with all she has ever known, will kiya return to egypt or give her life and her future to yahweh?\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"The backtranslated description retains the general structure and key thematic content of the original, effectively preserving the intent and meaning. It demonstrates moderate paraphrastic variation — some phrases are reformulated or slightly altered in wording, which indicates that the backtranslation process is introducing diversity.  \nHowever, the level of transformation remains relatively mild, with sections closely mirroring the original phrasing or being repeated with only small grammatical shifts.","metadata":{}},{"cell_type":"markdown","source":"## Data Augmentation: Paraphrasing\n\nParaphrasing is another data augmentation technique where input texts are rewritten in different words while preserving their original meaning.\n  \nWhen the training dataset is small, the model can easily overfit or fail to generalize to unseen descriptions. Paraphrasing helps address this by exposing the model to a wider variety of language patterns and sentence structures and by teaching the model to generate consistent outputs (titles) from semantically similar inputs (paraphrased descriptions).\n  \nWe are training a custom encoder-decoder transformer model and since we only have a limited number of (description, title) pairs, paraphrasing is applied only to the input descriptions — titles remain unchanged. This creates new synthetic training pairs where the input is varied, but the target stays the same.  \nThis teaches the model that multiple phrasings can map to the same correct title, reinforcing its understanding of meaning.","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:09:25.561881Z","iopub.execute_input":"2025-07-16T14:09:25.562495Z","iopub.status.idle":"2025-07-16T14:09:25.601556Z","shell.execute_reply.started":"2025-07-16T14:09:25.562470Z","shell.execute_reply":"2025-07-16T14:09:25.600822Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"In the following code, we load a pretrained T5 model fine-tuned for paraphrasing, specifically the **\"ramsrigouthamg/t5_paraphraser\" checkpoint**.\n  \nThe code performs automatic text paraphrasing by splitting the input into smaller chunks and generating paraphrased versions using a pre-trained language model. First, the same chunk_text function as before breaks the input text into sentence-based chunks, each constrained to a maximum word count (default 100 words), to ensure that the model input remains within acceptable limits. These chunks are then processed in the paraphrase_text function, where each one is prepended with a \"paraphrase:\" prompt and tokenized using a dedicated paraphrasing tokenizer. The model then generates paraphrased alternatives using sampling-based decoding. Specifically, the .generate() method is configured with several parameters that enhance output diversity, similar to the parameters used for backtranslation. Additionally the **num_return_sequences** parameter can generate several alternative paraphrases for each chunk. Although multiple options can be generated, only the first one is selected (outputs[0]). The generated tokens are decoded, postprocessed and combined into a final paraphrased version of the input text.","metadata":{}},{"cell_type":"code","source":"# Load the pretrained T5 model and tokenizer for paraphrasing\n# This model is trained specifically to generate paraphrases of input text.\nparaphrase_model = T5ForConditionalGeneration.from_pretrained(\"ramsrigouthamg/t5_paraphraser\")\nparaphrase_tokenizer = T5Tokenizer.from_pretrained(\"ramsrigouthamg/t5_paraphraser\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:09:27.656901Z","iopub.execute_input":"2025-07-16T14:09:27.657437Z","iopub.status.idle":"2025-07-16T14:09:33.574130Z","shell.execute_reply.started":"2025-07-16T14:09:27.657413Z","shell.execute_reply":"2025-07-16T14:09:33.573292Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c738d124ee15427e90d94cd3c2554a1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746f20de10844009bf033f828ca5c2df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70327143d0e0410ab7b8b3a5b610d3eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28bff8e0968148a586634de6206e99e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a562ad05a84171ab9af1aeb20aab17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58df2f3d17cb4feb9aa566f226365405"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Function to paraphrase text\ndef paraphrase_text(text, max_length=512, num_return_sequences=1):\n    try:\n        # Split the input text into smaller chunks to avoid exceeding model limits\n        chunks = chunk_text(text)  # Uses the chunk_text function defined earlier\n\n        paraphrased_chunks = []\n\n        for chunk in chunks:\n            # Prepare the input prompt for the paraphrasing model\n            prompt = \"paraphrase: \" + chunk.strip()\n\n            # Tokenize the input prompt\n            encoding = paraphrase_tokenizer.encode_plus(\n                prompt,\n                return_tensors=\"pt\",        # Return PyTorch tensors\n                truncation=True,            # Truncate input if it exceeds max_length\n                padding=\"max_length\",       # Pad input to the max_length\n                max_length=512              # Max number of tokens allowed in input\n            )\n\n            input_ids = encoding[\"input_ids\"]\n            attention_mask = encoding[\"attention_mask\"]\n\n            # Generate paraphrased output using a sampling-based strategy\n            outputs = paraphrase_model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=max_length,      # Maximum length of generated text\n                do_sample=True,             # Enables sampling for more diverse outputs\n                top_k=10,                   # Limits choices to top most probable tokens\n                top_p=0.1,                  # Nucleus sampling: top tokens with cumulative probability\n                temperature=1.9,            # Adds randomness: closer to 1.0 = more variation\n                num_return_sequences=num_return_sequences  # Generate multiple paraphrase options\n            )\n\n            # Select the first generated paraphrase\n            best_output = outputs[0]\n\n            # Decode token IDs to string and clean it\n            decoded = paraphrase_tokenizer.decode(best_output, skip_special_tokens=True).strip()\n\n            # Add the paraphrased chunk to the list\n            paraphrased_chunks.append(decoded)\n\n            # Combine all paraphrased chunks into final output\n            paraphrased_text = ' '.join(paraphrased_chunks)\n\n        # Text post-processing\n        return preprocess_text(paraphrased_text)\n\n    except Exception as e:\n        print(f\"Error during paraphrasing: {e}\")\n        return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:09:40.437670Z","iopub.execute_input":"2025-07-16T14:09:40.438414Z","iopub.status.idle":"2025-07-16T14:09:40.444378Z","shell.execute_reply.started":"2025-07-16T14:09:40.438388Z","shell.execute_reply":"2025-07-16T14:09:40.443747Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"The next code performs data augmentation via paraphrasing on the remaining 50% of the training samples that were not used for backtranslation.","metadata":{}},{"cell_type":"code","source":"# Select the remaining 50% of the training samples that were NOT used for backtranslation\npp_indices = list(set(range(num_samples)) - set(bt_indices))\n\n# Get the corresponding book descriptions and titles for paraphrasing\nsampled_descriptions = [train_descriptions[i] for i in pp_indices]\nsampled_titles = [train_titles[i] for i in pp_indices]\n\n# Lists to store augmented (paraphrased) data\naugmented_descriptions = []\naugmented_titles = []\n\n# Loop through each sample in the selected subset\nfor i in tqdm(range(len(pp_indices)), desc=\"Paraphrasing\"):\n    desc = sampled_descriptions[i]   # Original description\n    title = sampled_titles[i]        # Corresponding title (kept unchanged)\n\n    # Generate a paraphrased version of the description\n    new_desc = paraphrase_text(desc)\n\n    # Save the augmented description and the original title\n    augmented_descriptions.append(new_desc)\n    augmented_titles.append(title)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T14:09:41.835889Z","iopub.execute_input":"2025-07-16T14:09:41.836147Z","iopub.status.idle":"2025-07-16T15:45:45.927266Z","shell.execute_reply.started":"2025-07-16T14:09:41.836128Z","shell.execute_reply":"2025-07-16T15:45:45.926474Z"}},"outputs":[{"name":"stderr","text":"Paraphrasing: 100%|██████████| 399/399 [1:36:04<00:00, 14.45s/it]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Let's have a look at some paraphrased descriptions.","metadata":{}},{"cell_type":"code","source":"n = 3\n\nfor i in range(n):\n    idx = pp_indices[i]  \n\n    print(f\"\\n--- Example {i+1} ---\")\n    print(f\"Original Title:      {train_titles[idx]}\")\n    print(f\"Augmented Title:     {augmented_titles[i]}\")\n    print(f\"\\nOriginal Description:\\n{train_descriptions[idx]}\")\n    print(f\"\\nAugmented Description:\\n{augmented_descriptions[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:45:56.033600Z","iopub.execute_input":"2025-07-16T15:45:56.034139Z","iopub.status.idle":"2025-07-16T15:45:56.039029Z","shell.execute_reply.started":"2025-07-16T15:45:56.034116Z","shell.execute_reply":"2025-07-16T15:45:56.038276Z"}},"outputs":[{"name":"stdout","text":"\n--- Example 1 ---\nOriginal Title:      lowriders to the center of the earth (lowriders in space #2)\nAugmented Title:     lowriders to the center of the earth (lowriders in space #2)\n\nOriginal Description:\nthe lovable trio from the acclaimed lowriders in space are back! lupe impala, elirio malaria, and el chavo octopus are living their dream at last. they're the proud owners of their very own garage. but when their beloved cat genie goes missing, they need to do everything they can to find him. little do they know the trail will lead them to the realm of mictlantecuhtli, the the lovable trio from the acclaimed lowriders in space are back! lupe impala, elirio malaria, and el chavo octopus are living their dream at last. they're the proud owners of their very own garage. but when their beloved cat genie goes missing, they need to do everything they can to find him. little do they know the trail will lead them to the realm of mictlantecuhtli, the aztec god of the underworld, who is keeping genie prisoner! with cool spanish phrases on every page, a glossary of terms, and an action-packed plot that sneaks in science as well as aztec lore, lowriders to the center of the earth is a linguistic and visual delight. ¡que suave!\n\nAugmented Description:\nthe lovable trio from the acclaimed lowriders in space are back! lupe impala, elirio malaria, and el chavo octopus are living their dream at last. they're the proud owners of their very own garage. but when their beloved cat genie goes missing, they need to do everything they know the trail will lead them to the realm of mictlantecuhtli, the lovable trio from the acclaimed lowriders in when their cat genie goes missing, they need to do everything they can to find him. little do they know the trail will lead them to the realm of mictlantecuhtli, the aztec god of the underworld, who is keeping genie prisoner! with cool spanish phrases on every page, a glossary of terms, and an action-packed plot that sneaks in science as well as aztec lore, lowriders to the center of the earth is a linguistic\n\n--- Example 2 ---\nOriginal Title:      deep under (walker security #1)\nAugmented Title:     deep under (walker security #1)\n\nOriginal Description:\nalternate cover edition for 9781682302262myla is beautiful, a dove with clipped wings, captive by the wolf, a vicious drug lord. one look into her eyes and kyle could see the pain, the fear…the desperation. or so it seems. he's been fooled before by a woman and it cost him everything and everyone he loved. he won't be fooled again.tall, dark, and deadly, these are the stor alternate cover edition for 9781682302262myla is beautiful, a dove with clipped wings, captive by the wolf, a vicious drug lord. one look into her eyes and kyle could see the pain, the fear…the desperation. or so it seems. he's been fooled before by a woman and it cost him everything and everyone he loved. he won't be fooled again.tall, dark, and deadly, these are the stories of the men who run walker security. each man is unique in his methods and skills, but all share key similarities. they are passionate about those they love, relentless when fighting for a cause they believe in, and all believe that no case is too hard, no danger too dark. dedication is what they deliver, results are their reward.\n\nAugmented Description:\nalternate cover edition for 9781682302262myla is beautiful, a dove with clipped wings, captive by the wolf, a vicious drug lord. one look into her eyes and kyle could see the pain, the fear...the desperation. or so it seems. he's been fooled before by a woman and it cost him everything and everyone he loved. he won't be fooled again.tall, dark, and deadly, these are the he's been fooled before by a woman and it cost him everything and everyone he loved. he won't be fooled again.tall, dark, and deadly, these are the stories of the men who run walker security. each man is unique in his methods and skills, but all share key similarities. they are passionate about those they love, relentless when fighting for a cause they believe in, and all believe that no case is too hard, no danger too dark. dedication is what they deliver, results\n\n--- Example 3 ---\nOriginal Title:      the last mile (amos decker #2)\nAugmented Title:     the last mile (amos decker #2)\n\nOriginal Description:\nin his #1 new york times bestseller memory man, david baldacci introduced the extraordinary detective amos decker-the man who can forget nothing. now, decker returns in a spectacular new thriller . . . the last mileconvicted murderer melvin mars is counting down the last hours before his execution--for the violent killing of his parents twenty years earlier--when he's gran in his #1 new york times bestseller memory man, david baldacci introduced the extraordinary detective amos decker-the man who can forget nothing. now, decker returns in a spectacular new thriller . . . the last mileconvicted murderer melvin mars is counting down the last hours before his execution--for the violent killing of his parents twenty years earlier--when he's granted an unexpected reprieve. another man has confessed to the crime.amos decker, newly hired on an fbi special task force, takes an interest in mars's case after discovering the striking similarities to his own life: both men were talented football players with promising careers cut short by tragedy. both men's families were brutally murdered. and in both cases, another suspect came forward, years after the killing, to confess to the crime. a suspect who may or may not have been telling the truth.the confession has the potential to make melvin mars--guilty or not--a free man. who wants mars out of prison? and why now?but when a member of decker's team disappears, it becomes clear that something much larger--and more sinister--than just one convicted criminal's life hangs in the balance. decker will need all of his extraordinary brainpower to stop an innocent man from being executed.\n\nAugmented Description:\nin his #1 new york times bestseller memory man, david baldacci introduced the extraordinary detective amos decker-the man who can forget nothing. now, decker returns in a spectacular new thriller . . . the last mileconvicted murderer melvin mars is counting down the last hours before his execution--for the violent killing of his parents twenty years earlier--when he's gran in his #1 new york times bestseller memory man, david baldacci introduced the extraordinary detective amos convicted murderer melvin mars is counting down the last hours before his execution--for the violent killing of his parents twenty years earlier--when he's granted an unexpected reprieve. another man has confessed to the crime.amos decker, newly hired on an fbi special task force, takes an interest in mars's case after discovering the striking similarities to his own life: both men were talented football players with promising careers cut short by tragedy. both men's families were brutally murdered. and in a suspect who may or may not have been telling the truth.the confession has the potential to make melvin mars--guilty or not--a free man. who wants mars out of prison?and why now?but when a member of decker's team disappears, it becomes clear that something much larger--and more sinister--than just one convicted criminal's life hangs in the balance.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"The paraphrased description retains the main thematic elements and core information from the original, which is beneficial for data augmentation in low-resource transformer training scenarios. The structure demonstrates some linguistic variation, contributing to training diversity.  ","metadata":{}},{"cell_type":"markdown","source":"Let's put it all together.","metadata":{}},{"cell_type":"code","source":"# New training data\ntrain_descriptions_all = train_descriptions + augmented_descriptions + bt_descriptions\ntrain_titles_all = train_titles + augmented_titles + bt_titles\n\n# DataFrame with new training data\ndf_prep = pd.DataFrame({\n    'Title': train_titles_all,\n    'Description': train_descriptions_all\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:46:13.096623Z","iopub.execute_input":"2025-07-16T15:46:13.097125Z","iopub.status.idle":"2025-07-16T15:46:13.102694Z","shell.execute_reply.started":"2025-07-16T15:46:13.097104Z","shell.execute_reply":"2025-07-16T15:46:13.101988Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"We will now check the training, validation and test texts for any missing entries and remove them if necessary. Afterwards, the prepared DataFrames will be saved as CSV files.","metadata":{}},{"cell_type":"code","source":"# Create Dataframes\n# df_prep with new training data\ndf_prep = pd.DataFrame({\n    'Title': train_titles_all,\n    'Description': train_descriptions_all\n})\n\n# df_valid with validation Data\ndf_valid = pd.DataFrame({\n    'Valid_Title': valid_titles,\n    'Valid_Description': valid_descriptions\n})\n\n# df_test with test data\ndf_test = pd.DataFrame({\n    'Test_Title': test_titles,\n    'Test_Description': test_descriptions\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:46:16.258191Z","iopub.execute_input":"2025-07-16T15:46:16.258503Z","iopub.status.idle":"2025-07-16T15:46:16.264159Z","shell.execute_reply.started":"2025-07-16T15:46:16.258484Z","shell.execute_reply":"2025-07-16T15:46:16.263472Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Check training data for missing entries\ndf_prep[df_prep.isnull().any(axis=1)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:46:20.881672Z","iopub.execute_input":"2025-07-16T15:46:20.882473Z","iopub.status.idle":"2025-07-16T15:46:20.891570Z","shell.execute_reply.started":"2025-07-16T15:46:20.882447Z","shell.execute_reply":"2025-07-16T15:46:20.890956Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [Title, Description]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Function that removes rows with missing values from a DataFrame and prints the number of rows that were dropped\ndef drop_missing_and_report(df):\n    initial_rows = len(df)\n    df_cleaned = df.dropna()\n    dropped_rows = initial_rows - len(df_cleaned)\n    print(f\"Dropped {dropped_rows} rows with missing values.\")\n    return df_cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:46:22.873334Z","iopub.execute_input":"2025-07-16T15:46:22.873576Z","iopub.status.idle":"2025-07-16T15:46:22.877512Z","shell.execute_reply.started":"2025-07-16T15:46:22.873560Z","shell.execute_reply":"2025-07-16T15:46:22.876880Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Remove rows with missing entries\ndf_prep = drop_missing_and_report(df_prep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:46:23.654194Z","iopub.execute_input":"2025-07-16T15:46:23.654738Z","iopub.status.idle":"2025-07-16T15:46:23.660690Z","shell.execute_reply.started":"2025-07-16T15:46:23.654717Z","shell.execute_reply":"2025-07-16T15:46:23.659984Z"}},"outputs":[{"name":"stdout","text":"Dropped 0 rows with missing values.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Remove rows with missing entries\ndf_valid = drop_missing_and_report(df_valid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:46:25.493409Z","iopub.execute_input":"2025-07-16T15:46:25.493898Z","iopub.status.idle":"2025-07-16T15:46:25.498940Z","shell.execute_reply.started":"2025-07-16T15:46:25.493876Z","shell.execute_reply":"2025-07-16T15:46:25.498369Z"}},"outputs":[{"name":"stdout","text":"Dropped 0 rows with missing values.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Remove rows with missing entries\ndf_test = drop_missing_and_report(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:46:27.113340Z","iopub.execute_input":"2025-07-16T15:46:27.113868Z","iopub.status.idle":"2025-07-16T15:46:27.119101Z","shell.execute_reply.started":"2025-07-16T15:46:27.113847Z","shell.execute_reply":"2025-07-16T15:46:27.118293Z"}},"outputs":[{"name":"stdout","text":"Dropped 0 rows with missing values.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Save prepared Dataframes as CSV files\ndf_prep.to_csv('train_book_data.csv',index=False)\ndf_valid.to_csv('valid_book_data.csv',index=False)\ndf_test.to_csv('test_book_data.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:46:28.918118Z","iopub.execute_input":"2025-07-16T15:46:28.918419Z","iopub.status.idle":"2025-07-16T15:46:29.006531Z","shell.execute_reply.started":"2025-07-16T15:46:28.918401Z","shell.execute_reply":"2025-07-16T15:46:29.005923Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}